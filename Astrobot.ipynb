{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "import math\n",
    "from pandas import read_csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn import preprocessing\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from tensorflow.python.client import device_lib \n",
    "CUDA_VISIBLE_DEVICES = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff(pred, real, score, percent):\n",
    "    sampleNum, correct, cutoff = len(real), 1, 0.0001\n",
    "\n",
    "    while (sampleNum/len(real))>percent/100:\n",
    "        sampleNum, correct = 0,0\n",
    "        for i in range(len(pred)):\n",
    "            if score[i,argmax(score[i])]>cutoff:\n",
    "                sampleNum+=1\n",
    "                if real[i]==pred[i]:\n",
    "                    correct+=1\n",
    "        cutoff += 0.0001\n",
    "        \n",
    "    if sampleNum and correct !=0:\n",
    "        print(percent, \"Percent Test: \",sampleNum, correct, correct/sampleNum*100, \"%, Cutoff Value:\", cutoff)\n",
    "    else:\n",
    "        print(\"NaN\")\n",
    "        \n",
    "def ones(pred, real, score):\n",
    "    sampleNum, correct = 0, 0\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        if score[i,argmax(score[i])]>=1:\n",
    "            sampleNum+=1\n",
    "            if real[i]==pred[i]:\n",
    "                correct+=1\n",
    "        \n",
    "    if sampleNum and correct !=0:\n",
    "        print(\"Ones: \",sampleNum, correct, correct/sampleNum*100, \"%, Cutoff Value:\")\n",
    "    else:\n",
    "        print(\"NaN\")\n",
    "    \n",
    "def softmaxTest (yhat, y_test, probs):\n",
    "    sampleNum, correct = 0,0\n",
    "    print(\"Normal Test\")\n",
    "    for i in range(len(y_test)):\n",
    "        sampleNum+=1\n",
    "        if y_test[i]==yhat[i]:\n",
    "            correct+=1\n",
    "\n",
    "    if sampleNum and correct !=0:\n",
    "         print(sampleNum, correct, correct/sampleNum*100, \"%\")\n",
    "    else:\n",
    "         print(\"NaN\")\n",
    "    \n",
    "    print(\"Cutoff Softmax Test\")\n",
    "    cutoff(yhat, y_test, probs, 90)\n",
    "    cutoff(yhat, y_test, probs, 75)\n",
    "    cutoff(yhat, y_test, probs, 66.6667)\n",
    "    cutoff(yhat, y_test, probs, 50)\n",
    "    cutoff(yhat, y_test, probs, 33.3333)\n",
    "    cutoff(yhat, y_test, probs, 20)\n",
    "    ones(yhat, y_test, probs)\n",
    "           \n",
    "    \n",
    "    \n",
    "\n",
    "    true, false = 0, 0\n",
    "    tCounter, fCounter = 0, 0\n",
    "    for i in range(len(probs)):\n",
    "        if(yhat[i]!=y_test[i]):\n",
    "            false+=probs[i,argmax(probs[i])]\n",
    "            fCounter+=1\n",
    "            #print(probs[i,argmax(probs[i])],argmax(probs[i]), yhat[i],y_test[i])\n",
    "        else:\n",
    "            true+=probs[i,argmax(probs[i])]\n",
    "            tCounter+=1\n",
    "\n",
    "    print(\"True data average softmax value: \", true/tCounter)\n",
    "    print(\"False data average softmax value: \", false/fCounter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPipe3(path):\n",
    "    dataframe = read_csv(path, header=0)\n",
    "    pitches = dataframe.iloc[0:,0]\n",
    "    pitches_List = [\"Fastball\", \"Breaking Ball\", \"Offspeed\", \"Other\"]\n",
    "    counter = 0\n",
    "\n",
    "    dl_Length = len(pitches)\n",
    "    num_Pitches = len(pitches_List)\n",
    "    \n",
    "    dataset = np.zeros([dl_Length,num_Pitches*8+23])\n",
    "    x_Length,y_Length = np.shape(dataset)\n",
    "    for i in range(len(pitches)):\n",
    "        if pitches[i] == \"FF\" or  pitches[i] == \"FT\" or pitches[i] == \"FS\" or pitches[i] ==  \"FC\" or pitches[i] ==  \"SI\": # fastball\n",
    "            dataset[i,y_Length-1] = 0\n",
    "        elif pitches[i] == \"CU\" or pitches[i] ==  \"SL\" or pitches[i] == \"KC\" or pitches[i] == \"EP\": # breaking ball\n",
    "            dataset[i,y_Length-1] = 1\n",
    "        elif pitches[i] == \"CH\" or pitches[i] == \"SF\" or pitches[i] == \"SC\" or pitches[i] == \"FO\" or pitches[i] == \"KN\": # offspeed\n",
    "            dataset[i,y_Length-1] = 2\n",
    "        else:\n",
    "            dataset[i,y_Length-1] = 3 \n",
    "                \n",
    "    for i in range(dl_Length):\n",
    "        # date\n",
    "        dataset[i,0] = int(str(dataframe.iloc[i,1]) .replace(\"-\",\"\"))\n",
    "        # batter ID\n",
    "        dataset[i,1] = int(dataframe.iloc[i,6])\n",
    "        #pitcher ID\n",
    "        dataset[i,2] = int(dataframe.iloc[i,7])\n",
    "\n",
    "\n",
    "        # R is 0 and L is one\n",
    "        if(dataframe.iloc[i,16] == \"R\"):\n",
    "            dataset[i,3] = 0\n",
    "        elif(dataframe.iloc[i,16] == \"L\"):\n",
    "            dataset[i,3] = 1\n",
    "\n",
    "        # R is 0 and L is one\n",
    "        if(dataframe.iloc[i,17] == \"R\"):\n",
    "            dataset[i,4] = 0\n",
    "        elif(dataframe.iloc[i,17] == \"L\"):\n",
    "            dataset[i,4] = 1\n",
    "\n",
    "        # R is 0 and L is one\n",
    "        if(dataframe.iloc[i,18] == \"R\"):\n",
    "            dataset[i,5] = 0\n",
    "        elif(dataframe.iloc[i,18] == \"L\"):\n",
    "            dataset[i,5] = 1\n",
    "\n",
    "        #Ball  \n",
    "        dataset[i,6] = int(dataframe.iloc[i,24])\n",
    "        #Strike\n",
    "        dataset[i,7] = int(dataframe.iloc[i,25])\n",
    "\n",
    "        # On base\n",
    "        # 3B\n",
    "        if(math.isnan(dataframe.iloc[i,31])):\n",
    "\n",
    "            dataset[i,8] = 0\n",
    "        else:\n",
    "            dataset[i,8] = 1\n",
    "\n",
    "\n",
    "        #2B   \n",
    "        if(math.isnan(dataframe.iloc[i,31])):\n",
    "            dataset[i,9] = 0\n",
    "        else:\n",
    "            dataset[i,9] = 1\n",
    "        #1B\n",
    "        if(math.isnan(dataframe.iloc[i,31])):\n",
    "            dataset[i,10] = 0\n",
    "        else:\n",
    "            dataset[i,10] = 1\n",
    "\n",
    "        # innings\n",
    "        dataset[i,11] = int(dataframe.iloc[i,35])\n",
    "\n",
    "        # outs\n",
    "        dataset[i,12] = int(dataframe.iloc[i,34])\n",
    "\n",
    "        # Catcher\n",
    "        if(math.isnan(dataframe.iloc[i,60])):\n",
    "            dataset[i,13] = 0\n",
    "        else:\n",
    "            dataset[i,13] = int(dataframe.iloc[i,60])\n",
    "\n",
    "        # Score\n",
    "        #bat score\n",
    "        dataset[i,14] = int(dataframe.iloc[i,81])\n",
    "        #Field Score\n",
    "        dataset[i,15] = int(dataframe.iloc[i,82])\n",
    "\n",
    "        # Year/Career Pitches number\n",
    "        dataset[i,16] = dl_Length-1-i\n",
    "\n",
    "        dataset[dl_Length-1,17] = 0\n",
    "        dataset[dl_Length-1,18] = 0\n",
    "        dataset[dl_Length-1,19] = 0\n",
    "        dataset[dl_Length-1,20:y_Length-4]=0\n",
    "        dataset[dl_Length-1,y_Length-3] = -1\n",
    "        dataset[dl_Length-1, y_Length-2] = 1\n",
    "        dataset[dl_Length-2, y_Length-2] = 1\n",
    "\n",
    "    # game, out and inning pitch count \n",
    "    for i in range(dl_Length-2,-1,-1):\n",
    "        # streak type\n",
    "        dataset[i,y_Length-3] = dataset[i+1,y_Length-1]\n",
    "        # game\n",
    "        if dataset[i,0]!=dataset[i+1,0]:\n",
    "            dataset[i,17] = 0\n",
    "        else:\n",
    "            dataset[i,17] = dataset[i+1,17]+1\n",
    "\n",
    "        # inning\n",
    "        if dataset[i,11]!=dataset[i+1,11]:\n",
    "            dataset[i,18] = 0\n",
    "        else:\n",
    "            dataset[i,18] = dataset[i+1,18]+1\n",
    "\n",
    "        # out \n",
    "        if dataset[i,1]!=dataset[i+1,1]:\n",
    "            dataset[i,19] = 0\n",
    "        else:\n",
    "            dataset[i,19] = dataset[i+1,19]+1\n",
    "\n",
    "        # year/career pitch tracker\n",
    "        dataset[i,20:20+num_Pitches] = dataset[i+1,20:20+num_Pitches]\n",
    "        dataset[i,20+int(dataset[i+1,y_Length-1])]+=1\n",
    "\n",
    "        # percentages\n",
    "        if dataset[i,16]==0:\n",
    "            dataset[i,20+num_Pitches:20+num_Pitches*2] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches:20+num_Pitches*2] = dataset[i,20:20+num_Pitches]/dataset[i,16]\n",
    "\n",
    "        #game pitch tracker\n",
    "        if dataset[i+1,0]!=dataset[i,0]:\n",
    "            dataset[i,20+num_Pitches*2:20+num_Pitches*3] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*2:20+num_Pitches*3] = dataset[i+1,20+num_Pitches*2:20+num_Pitches*3]\n",
    "            dataset[i,20+num_Pitches*2+int(dataset[i+1,y_Length-1])]+=1\n",
    "\n",
    "         # percentages\n",
    "        if dataset[i,17]==0:\n",
    "            dataset[i,20+num_Pitches*3:20+num_Pitches*4] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*3:20+num_Pitches*4] = dataset[i,20+num_Pitches*2:20+num_Pitches*3]/dataset[i,17]\n",
    "\n",
    "        #inning pitch tracker\n",
    "        if dataset[i+1,11]!=dataset[i,11]:\n",
    "            dataset[i,20+num_Pitches*4:20+num_Pitches*5] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*4:20+num_Pitches*5] = dataset[i+1,20+num_Pitches*4:20+num_Pitches*5]\n",
    "            dataset[i,20+num_Pitches*4+int(dataset[i+1,y_Length-1])]+=1\n",
    "\n",
    "         # percentages\n",
    "        if dataset[i,18]==0:\n",
    "            dataset[i,20+num_Pitches*5:20+num_Pitches*6] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*5:20+num_Pitches*6] = dataset[i,20+num_Pitches*4:20+num_Pitches*5]/dataset[i,18]\n",
    "\n",
    "\n",
    "        #out pitch tracker\n",
    "\n",
    "        if dataset[i+1,1]!=dataset[i,1]:\n",
    "            dataset[i,20+num_Pitches*6:20+num_Pitches*7] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*6:20+num_Pitches*7] = dataset[i+1,20+num_Pitches*6:20+num_Pitches*7]\n",
    "            dataset[i,20+num_Pitches*6+int(dataset[i+1,y_Length-1])]+=1\n",
    "\n",
    "         # percentages\n",
    "        if dataset[i,19]==0:\n",
    "            dataset[i,20+num_Pitches*7:20+num_Pitches*8] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*7:20+num_Pitches*8] = dataset[i,20+num_Pitches*6:20+num_Pitches*7]/dataset[i,19]  \n",
    "            \n",
    "    # streak length\n",
    "    for i in range(dl_Length-3,-1,-1):\n",
    "        if dataset[i,y_Length-1] == dataset[i+1,y_Length-1]:\n",
    "            dataset[i,y_Length-2] = dataset[i+1,y_Length-2] +1\n",
    "    \n",
    "    np.savetxt(\"C:\\\\Users\\\\16479\\\\Desktop\\\\foo.csv\", dataset, delimiter=\",\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def dataPipe(path):\n",
    "    dataframe = read_csv(path, header=0)\n",
    "    pitches = dataframe.iloc[0:,0]\n",
    "    pitches_List = []\n",
    "    counter = 0\n",
    "\n",
    "    dl_Length = len(pitches)\n",
    "\n",
    "    for i in range(len(pitches)):\n",
    "        exists = False\n",
    "        j = 0 \n",
    "        for j in range(len(pitches_List)):\n",
    "            if pitches[i] == pitches_List[j]:\n",
    "                exists = True\n",
    "        if exists == False:\n",
    "            pitches_List.append(pitches[i])\n",
    "            counter+=1\n",
    "\n",
    "\n",
    "    num_Pitches = len(pitches_List)\n",
    "\n",
    "    dataset = np.zeros([dl_Length,num_Pitches*8+23])\n",
    "    x_Length,y_Length = np.shape(dataset)\n",
    "    for i in range(len(pitches)):\n",
    "        for j in range(len(pitches_List)):\n",
    "             if pitches[i] == pitches_List[j]:\n",
    "                dataset[i,y_Length-1] = j \n",
    "    for i in range(dl_Length):\n",
    "        # date\n",
    "        dataset[i,0] = int(str(dataframe.iloc[i,1]) .replace(\"-\",\"\"))\n",
    "        # batter ID\n",
    "        dataset[i,1] = int(dataframe.iloc[i,6])\n",
    "        #pitcher ID\n",
    "        dataset[i,2] = int(dataframe.iloc[i,7])\n",
    "\n",
    "\n",
    "        # R is 0 and L is one\n",
    "        if(dataframe.iloc[i,16] == \"R\"):\n",
    "            dataset[i,3] = 0\n",
    "        elif(dataframe.iloc[i,16] == \"L\"):\n",
    "            dataset[i,3] = 1\n",
    "\n",
    "        # R is 0 and L is one\n",
    "        if(dataframe.iloc[i,17] == \"R\"):\n",
    "            dataset[i,4] = 0\n",
    "        elif(dataframe.iloc[i,17] == \"L\"):\n",
    "            dataset[i,4] = 1\n",
    "\n",
    "        # R is 0 and L is one\n",
    "        if(dataframe.iloc[i,18] == \"R\"):\n",
    "            dataset[i,5] = 0\n",
    "        elif(dataframe.iloc[i,18] == \"L\"):\n",
    "            dataset[i,5] = 1\n",
    "\n",
    "        #Ball  \n",
    "        dataset[i,6] = int(dataframe.iloc[i,24])\n",
    "        #Strike\n",
    "        dataset[i,7] = int(dataframe.iloc[i,25])\n",
    "\n",
    "        # On base\n",
    "        # 3B\n",
    "        if(math.isnan(dataframe.iloc[i,31])):\n",
    "\n",
    "            dataset[i,8] = 0\n",
    "        else:\n",
    "            dataset[i,8] = 1\n",
    "\n",
    "\n",
    "        #2B   \n",
    "        if(math.isnan(dataframe.iloc[i,31])):\n",
    "            dataset[i,9] = 0\n",
    "        else:\n",
    "            dataset[i,9] = 1\n",
    "        #1B\n",
    "        if(math.isnan(dataframe.iloc[i,31])):\n",
    "            dataset[i,10] = 0\n",
    "        else:\n",
    "            dataset[i,10] = 1\n",
    "\n",
    "        # innings\n",
    "        dataset[i,11] = int(dataframe.iloc[i,35])\n",
    "\n",
    "        # outs\n",
    "        dataset[i,12] = int(dataframe.iloc[i,34])\n",
    "\n",
    "        # Catcher\n",
    "        if(math.isnan(dataframe.iloc[i,60])):\n",
    "            dataset[i,13] = 0\n",
    "        else:\n",
    "            dataset[i,13] = int(dataframe.iloc[i,60])\n",
    "\n",
    "        # Score\n",
    "        #bat score\n",
    "        dataset[i,14] = int(dataframe.iloc[i,81])\n",
    "        #Field Score\n",
    "        dataset[i,15] = int(dataframe.iloc[i,82])\n",
    "\n",
    "        # Year/Career Pitches number\n",
    "        dataset[i,16] = dl_Length-1-i\n",
    "\n",
    "        dataset[dl_Length-1,17] = 0\n",
    "        dataset[dl_Length-1,18] = 0\n",
    "        dataset[dl_Length-1,19] = 0\n",
    "        dataset[dl_Length-1,20:y_Length-4]=0\n",
    "        dataset[dl_Length-1,y_Length-3] = -1\n",
    "        dataset[dl_Length-1, y_Length-2] = 1\n",
    "        dataset[dl_Length-2, y_Length-2] = 1\n",
    "\n",
    "    # game, out and inning pitch count \n",
    "    for i in range(dl_Length-2,-1,-1):\n",
    "        # streak type\n",
    "        dataset[i,y_Length-3] = dataset[i+1,y_Length-1]\n",
    "        # game\n",
    "        if dataset[i,0]!=dataset[i+1,0]:\n",
    "            dataset[i,17] = 0\n",
    "        else:\n",
    "            dataset[i,17] = dataset[i+1,17]+1\n",
    "\n",
    "        # inning\n",
    "        if dataset[i,11]!=dataset[i+1,11]:\n",
    "            dataset[i,18] = 0\n",
    "        else:\n",
    "            dataset[i,18] = dataset[i+1,18]+1\n",
    "\n",
    "        # out \n",
    "        if dataset[i,1]!=dataset[i+1,1]:\n",
    "            dataset[i,19] = 0\n",
    "        else:\n",
    "            dataset[i,19] = dataset[i+1,19]+1\n",
    "\n",
    "        # year/career pitch tracker\n",
    "        dataset[i,20:20+num_Pitches] = dataset[i+1,20:20+num_Pitches]\n",
    "        dataset[i,20+int(dataset[i+1,y_Length-1])]+=1\n",
    "\n",
    "        # percentages\n",
    "        if dataset[i,16]==0:\n",
    "            dataset[i,20+num_Pitches:20+num_Pitches*2] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches:20+num_Pitches*2] = dataset[i,20:20+num_Pitches]/dataset[i,16]\n",
    "\n",
    "        #game pitch tracker\n",
    "        if dataset[i+1,0]!=dataset[i,0]:\n",
    "            dataset[i,20+num_Pitches*2:20+num_Pitches*3] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*2:20+num_Pitches*3] = dataset[i+1,20+num_Pitches*2:20+num_Pitches*3]\n",
    "            dataset[i,20+num_Pitches*2+int(dataset[i+1,y_Length-1])]+=1\n",
    "\n",
    "         # percentages\n",
    "        if dataset[i,17]==0:\n",
    "            dataset[i,20+num_Pitches*3:20+num_Pitches*4] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*3:20+num_Pitches*4] = dataset[i,20+num_Pitches*2:20+num_Pitches*3]/dataset[i,17]\n",
    "\n",
    "        #inning pitch tracker\n",
    "        if dataset[i+1,11]!=dataset[i,11]:\n",
    "            dataset[i,20+num_Pitches*4:20+num_Pitches*5] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*4:20+num_Pitches*5] = dataset[i+1,20+num_Pitches*4:20+num_Pitches*5]\n",
    "            dataset[i,20+num_Pitches*4+int(dataset[i+1,y_Length-1])]+=1\n",
    "\n",
    "         # percentages\n",
    "        if dataset[i,18]==0:\n",
    "            dataset[i,20+num_Pitches*5:20+num_Pitches*6] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*5:20+num_Pitches*6] = dataset[i,20+num_Pitches*4:20+num_Pitches*5]/dataset[i,18]\n",
    "\n",
    "\n",
    "        #out pitch tracker\n",
    "\n",
    "        if dataset[i+1,1]!=dataset[i,1]:\n",
    "            dataset[i,20+num_Pitches*6:20+num_Pitches*7] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*6:20+num_Pitches*7] = dataset[i+1,20+num_Pitches*6:20+num_Pitches*7]\n",
    "            dataset[i,20+num_Pitches*6+int(dataset[i+1,y_Length-1])]+=1\n",
    "\n",
    "         # percentages\n",
    "        if dataset[i,19]==0:\n",
    "            dataset[i,20+num_Pitches*7:20+num_Pitches*8] = 0\n",
    "        else:\n",
    "            dataset[i,20+num_Pitches*7:20+num_Pitches*8] = dataset[i,20+num_Pitches*6:20+num_Pitches*7]/dataset[i,19]  \n",
    "            \n",
    "    # streak length\n",
    "    for i in range(dl_Length-3,-1,-1):\n",
    "        if dataset[i,y_Length-1] == dataset[i+1,y_Length-1]:\n",
    "            dataset[i,y_Length-2] = dataset[i+1,y_Length-2] +1\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# split data into train and test sets\n",
    "\n",
    "\n",
    "def modelRun(data, e, bs, v):\n",
    "    #np.savetxt(\"C:\\\\Users\\\\16479\\\\Desktop\\\\foo.csv\", dataset, delimiter=\",\")\n",
    "    # split into input (X) and output (y) variables\n",
    "    #dataframe = read_csv(\"C:\\\\Users\\\\16479\\\\Desktop\\\\Matz_Audible.csv\", header=0)\n",
    "    #dataset = dataframe.values\n",
    "    data_X, data_y = data[:, 1:-1], data[:, -1]\n",
    "    data_X, data_y = data_X.astype('float'), data_y.astype('float')\n",
    "    n_features = data_X.shape[1]\n",
    "    data_X = preprocessing.scale(data_X)\n",
    "    #X_train, X_test, y_train, y_test = data_X[113:], data_X[0:113], data_y[113:], data_y[0:113]\n",
    "    X_train, X_test, y_train, y_test =train_test_split(data_X, data_y, test_size=0.2, random_state=2)\n",
    "\n",
    "    data_y = LabelEncoder().fit_transform(data_y)\n",
    "    n_class = len(unique(data_y))\n",
    "    \n",
    "    configuration = tf.compat.v1.ConfigProto()\n",
    "    configuration.gpu_options.allow_growth = True\n",
    "    session = tf.compat.v1.Session(config=configuration)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(90, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(70, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=\"RMSprop\")\n",
    "    # fit the keras model on the dataset\n",
    "    #Optimizers: adam, SGD, RMSprop, adagard\n",
    "    #loss: sparse categorical crossentropy,\n",
    "    #different batch sizes\n",
    "\n",
    "    history=model.fit(X_train, y_train, epochs = e, batch_size = bs, verbose = v)\n",
    " \n",
    "    # evaluate on test set\n",
    "    yhat = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    yhat = argmax(yhat, axis=-1).astype('int')\n",
    "    acc = accuracy_score(y_test, yhat)\n",
    "    return acc, yhat, y_test, probs, model\n",
    "    #print('Accuracy: %.3f' % acc)\n",
    "    \n",
    "def modelRunTest(data, test, e, bs, v):\n",
    "    data_X, data_y = data[:, 1:-1], data[:, -1]\n",
    "    data_X, data_y = data_X.astype('float'), data_y.astype('float')\n",
    "    \n",
    "    data_XT, data_yT = test[:, 1:-1], test[:, -1]\n",
    "    data_XT, data_yT = data_XT.astype('float'), data_yT.astype('float')\n",
    "    \n",
    "    n_features = data_X.shape[1]\n",
    "    data_X = preprocessing.scale(data_X)\n",
    "    data_XT = preprocessing.scale(data_XT)\n",
    "    X_train, X_test, y_train, y_test = data_X, data_XT, data_y, data_yT\n",
    "\n",
    "    data_y = LabelEncoder().fit_transform(data_y)\n",
    "    n_class = len(unique(data_y))\n",
    "    \n",
    "    configuration = tf.compat.v1.ConfigProto()\n",
    "    configuration.gpu_options.allow_growth = True\n",
    "    session = tf.compat.v1.Session(config=configuration)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(90, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(70, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=\"RMSprop\")\n",
    "    # fit the keras model on the dataset\n",
    "    #Optimizers: adam, SGD, RMSprop, adagard\n",
    "    #loss: sparse categorical crossentropy,\n",
    "    #different batch sizes\n",
    "\n",
    "    history=model.fit(X_train, y_train, epochs = e, batch_size = bs, verbose = v)\n",
    " \n",
    "    # evaluate on test set\n",
    "    yhat = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    yhat = argmax(yhat, axis=-1).astype('int')\n",
    "    acc = accuracy_score(y_test, yhat) \n",
    "    return acc, yhat, y_test, probs, model\n",
    "\n",
    "def modelRunTestSingle(data, test, e, bs, v):\n",
    "    x,y = np.shape(test)\n",
    "    data_X, data_y = data[:, 1:-1], data[:, -1]\n",
    "    data_X, data_y = data_X.astype('float'), data_y.astype('float')\n",
    "    \n",
    "    data_XT, data_yT = test[:, 1:y-1], test[:, y-1]\n",
    "    data_XT, data_yT = data_XT.astype('float'), data_yT.astype('float')\n",
    "    \n",
    "    n_features = data_X.shape[1]\n",
    "    data_X = preprocessing.scale(data_X)\n",
    "    data_XT = preprocessing.scale(data_XT)\n",
    "    X_train, X_test, y_train, y_test = data_X, data_XT, data_y, data_yT\n",
    "\n",
    "    data_y = LabelEncoder().fit_transform(data_y)\n",
    "    n_class = len(unique(data_y))\n",
    "    \n",
    "    configuration = tf.compat.v1.ConfigProto()\n",
    "    configuration.gpu_options.allow_growth = True\n",
    "    session = tf.compat.v1.Session(config=configuration)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(90, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(70, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=\"RMSprop\")\n",
    "    # fit the keras model on the dataset\n",
    "    #Optimizers: adam, SGD, RMSprop, adagard\n",
    "    #loss: sparse categorical crossentropy,\n",
    "    #different batch sizes\n",
    "\n",
    "    history=model.fit(X_train, y_train, epochs = e, batch_size = bs, verbose = v)\n",
    " \n",
    "    # evaluate on test set\n",
    "    yhat = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    yhat = argmax(yhat, axis=-1).astype('int')\n",
    "    acc = accuracy_score(y_test, yhat) \n",
    "    return acc, yhat, y_test, probs, model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def modelRunUpdateTest(data, test, epoch, batch, verb):# finish this\n",
    "    length, width = np.shape(test)\n",
    "    guesses = np.zeros((length))\n",
    "    pitches = np.zeros((length))\n",
    "    for i in range(len(test)):\n",
    "        tData = np.zeros((1,width))\n",
    "        tData[:,0:width] = test[length-i-1]\n",
    "        \n",
    "        \n",
    "        acc, preds, real, probs, model = modelRunTestSingle(data, tData, epoch, batch, verb)\n",
    "        data = np.append(data, tData, axis = 0)\n",
    "        guesses[i] = preds\n",
    "        pitches[i] = real\n",
    "\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for i in range(len(guesses)):\n",
    "        if guesses[i]==pitches[i]:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "    print(correct/total, \" %\")\n",
    "    return acc, guesses, pitches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46875  %\n"
     ]
    }
   ],
   "source": [
    "one, two, three = modelRunUpdateTest(d3a, t3, 20, 32, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7101449275362319 % epochs:  5\n",
      "0.6589285714285713 % epochs:  5\n",
      "0.7020833333333333 % epochs:  5\n",
      "0.8545454545454545 % epochs:  5\n",
      "0.791304347826087 % epochs:  5\n",
      "\n",
      "\n",
      "0.6608695652173913 % epochs:  10\n",
      "0.6285714285714287 % epochs:  10\n",
      "0.7208333333333332 % epochs:  10\n",
      "0.6545454545454545 % epochs:  10\n",
      "0.7652173913043477 % epochs:  10\n",
      "\n",
      "\n",
      "0.672463768115942 % epochs:  15\n",
      "0.7000000000000001 % epochs:  15\n",
      "0.7333333333333333 % epochs:  15\n",
      "0.6 % epochs:  15\n",
      "0.6956521739130436 % epochs:  15\n",
      "\n",
      "\n",
      "0.6521739130434783 % epochs:  20\n",
      "0.7017857142857142 % epochs:  20\n"
     ]
    }
   ],
   "source": [
    "t1 = d1[0:69]\n",
    "d1a =  d1[69:]\n",
    "fd1 = d1a\n",
    "dSet1 = [d1a, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11]\n",
    "\n",
    "for i in range(1,11):\n",
    "    fd1 = np.concatenate((fd1, dSet1[i]), axis=0)\n",
    "\n",
    "t2 = d2[0:112]\n",
    "d2a =  d2[112:]\n",
    "fd2 = d2a\n",
    "dSet2 = [d2a, d1, d3, d4, d5, d6, d7, d8, d9, d10, d11]\n",
    "\n",
    "for i in range(1,11):\n",
    "    fd2 = np.concatenate((fd2, dSet2[i]), axis=0)\n",
    "    \n",
    "t3 = d3[0:96]\n",
    "d3a =  d3[96:]\n",
    "fd3 = d3a\n",
    "dSet3 = [d3a, d1, d2, d4, d5, d6, d7, d8, d9, d10, d11]\n",
    "\n",
    "for i in range(1,11):\n",
    "    fd3 = np.concatenate((fd3, dSet3[i]), axis=0)\n",
    "    \n",
    "t4 = d4[0:11]\n",
    "d4a =  d4[11:]\n",
    "fd4 = d4a\n",
    "dSet4 = [d4a, d1, d2, d3, d5, d6, d7, d8, d9, d10, d11]\n",
    "\n",
    "for i in range(1,11):\n",
    "    fd4 = np.concatenate((fd4, dSet4[i]), axis=0)\n",
    "    \n",
    "t5 = d5[0:23]\n",
    "d5a =  d5[23:]\n",
    "fd5 = d5a\n",
    "dSet5 = [d5a, d1, d2, d4, d5, d6, d7, d8, d9, d10, d11]\n",
    "\n",
    "for i in range(1,11):\n",
    "    fd5 = np.concatenate((fd5, dSet5[i]), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in range(1,11):\n",
    "    total=0\n",
    "    for j in range(5):\n",
    "        four, five, six, seven, eight = modelRunTest(fd1, t1, i*5, 32, 0)\n",
    "        total+=four\n",
    "    print(total/5, \"% epochs: \", i*5)\n",
    "    \n",
    "    total=0\n",
    "    for j in range(5):\n",
    "        four, five, six, seven, eight = modelRunTest(fd2, t2, i*5, 32, 0)\n",
    "        total+=four\n",
    "    print(total/5, \"% epochs: \", i*5)\n",
    "    \n",
    "    total=0\n",
    "    for j in range(5):\n",
    "        four, five, six, seven, eight = modelRunTest(fd3, t3, i*5, 32, 0)\n",
    "        total+=four\n",
    "    print(total/5, \"% epochs: \", i*5)\n",
    "    \n",
    "    total=0\n",
    "    for j in range(5):\n",
    "        four, five, six, seven, eight = modelRunTest(fd4, t4, i*5, 32, 0)\n",
    "        total+=four\n",
    "    print(total/5, \"% epochs: \", i*5)\n",
    "    \n",
    "    total=0\n",
    "    for j in range(5):\n",
    "        four, five, six, seven, eight = modelRunTest(fd5, t5, i*5, 32, 0)\n",
    "        total+=four\n",
    "    print(total/5, \"% epochs: \", i*5)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Train on 7576 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Accuracy is  66.33928571428571   % on  5  epochs.\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train on 7576 samples\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Accuracy is  70.0   % on  10  epochs.\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n",
      "Train on 7576 samples\n",
      "Epoch 1/15\n",
      "Epoch 2/15\n",
      "Epoch 3/15\n",
      "Epoch 4/15\n",
      "Epoch 5/15\n",
      "Epoch 6/15\n",
      "Epoch 7/15\n",
      "Epoch 8/15\n",
      "Epoch 9/15\n",
      "Epoch 10/15\n",
      "Epoch 11/15\n",
      "Epoch 12/15\n",
      "Epoch 13/15\n",
      "Epoch 14/15\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-09ac900e561a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfour\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelRunTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy is \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"  % on \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" epochs.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-201-6cbf8005861a>\u001b[0m in \u001b[0;36mmodelRunTest\u001b[1;34m(data, test, e, bs, v)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;31m#different batch sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;31m# evaluate on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\GeneralEnvironment\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(1,11):   \n",
    "    total = 0\n",
    "    for i in range(10):\n",
    "        one, two, three, four, five = modelRunTest(fd,t, 5*j, 32, 3) \n",
    "        total+=one\n",
    "    print(\"Accuracy is \",total*10, \"  % on \", 5*j, \" epochs.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "p1 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (12).csv\"\n",
    "p2 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (5).csv\"\n",
    "p3 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (6).csv\"\n",
    "p4 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (7).csv\"\n",
    "p5 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (8).csv\"\n",
    "p6 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (9).csv\"\n",
    "p7 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (10).csv\"\n",
    "p8 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (11).csv\"\n",
    "p9 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (13).csv\"\n",
    "p10 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (14).csv\"\n",
    "p11 = \"C:\\\\Users\\\\16479\\\\Desktop\\\\Astrobot Data\\\\Old Data\\\\savant_data (15).csv\"\n",
    "\n",
    "d1 = dataPipe3(p1)\n",
    "d2 = dataPipe3(p2)\n",
    "d3 = dataPipe3(p3)\n",
    "d4 = dataPipe3(p4)\n",
    "d5 = dataPipe3(p5)\n",
    "d6 = dataPipe3(p6)\n",
    "d7 = dataPipe3(p7)\n",
    "d8 = dataPipe3(p8)\n",
    "d9 = dataPipe3(p9)\n",
    "d10 = dataPipe3(p10)\n",
    "d11 = dataPipe3(p11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  78.125   % on  85  epochs.\n",
      "Accuracy is  77.60416666666669   % on  90  epochs.\n",
      "Accuracy is  83.125   % on  95  epochs.\n",
      "Accuracy is  78.74999999999999   % on  100  epochs.\n",
      "Accuracy is  79.68750000000001   % on  105  epochs.\n",
      "Accuracy is  78.02083333333333   % on  110  epochs.\n",
      "Accuracy is  79.89583333333333   % on  115  epochs.\n",
      "Accuracy is  81.14583333333334   % on  120  epochs.\n",
      "Accuracy is  81.45833333333331   % on  125  epochs.\n",
      "Accuracy is  81.77083333333334   % on  130  epochs.\n",
      "Accuracy is  81.25   % on  135  epochs.\n",
      "Accuracy is  79.58333333333334   % on  140  epochs.\n",
      "Accuracy is  78.54166666666667   % on  145  epochs.\n"
     ]
    }
   ],
   "source": [
    "dTrain = d3[96:]\n",
    "dTest = d3[0:96]\n",
    "j,i=0,0\n",
    "for j in range(17,30):   \n",
    "    total = 0\n",
    "    for i in range(10):\n",
    "        one, two, three, four, five = modelRunTest(dTrain, dTest, 5*j, 32, 0) \n",
    "        total+=one\n",
    "    print(\"Accuracy is \",total*10, \"  % on \", 5*j, \" epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  20.625  % Global:  0  epochs\n",
      "Accuracy:  25.208333333333332  % Local:  0  epochs\n",
      "Accuracy:  71.45833333333333  % Global:  5  epochs\n",
      "Accuracy:  59.375  % Local:  5  epochs\n",
      "Accuracy:  78.125  % Global:  10  epochs\n",
      "Accuracy:  63.125  % Local:  10  epochs\n",
      "Accuracy:  80.83333333333333  % Global:  15  epochs\n",
      "Accuracy:  70.0  % Local:  15  epochs\n",
      "Accuracy:  74.16666666666666  % Global:  20  epochs\n",
      "Accuracy:  73.54166666666667  % Local:  20  epochs\n",
      "Accuracy:  75.83333333333334  % Global:  25  epochs\n",
      "Accuracy:  71.45833333333333  % Local:  25  epochs\n",
      "Accuracy:  67.70833333333333  % Global:  30  epochs\n",
      "Accuracy:  78.54166666666667  % Local:  30  epochs\n",
      "Accuracy:  79.16666666666667  % Global:  35  epochs\n",
      "Accuracy:  75.83333333333334  % Local:  35  epochs\n",
      "Accuracy:  68.75  % Global:  40  epochs\n",
      "Accuracy:  73.125  % Local:  40  epochs\n",
      "Accuracy:  65.83333333333334  % Global:  45  epochs\n",
      "Accuracy:  79.79166666666666  % Local:  45  epochs\n",
      "Accuracy:  77.5  % Global:  50  epochs\n",
      "Accuracy:  78.54166666666667  % Local:  50  epochs\n",
      "Accuracy:  68.75  % Global:  55  epochs\n",
      "Accuracy:  79.79166666666666  % Local:  55  epochs\n",
      "Accuracy:  75.41666666666667  % Global:  60  epochs\n",
      "Accuracy:  82.29166666666667  % Local:  60  epochs\n",
      "Accuracy:  81.04166666666666  % Global:  65  epochs\n",
      "Accuracy:  79.16666666666666  % Local:  65  epochs\n",
      "Accuracy:  79.58333333333333  % Global:  70  epochs\n",
      "Accuracy:  78.95833333333333  % Local:  70  epochs\n",
      "Accuracy:  72.08333333333334  % Global:  75  epochs\n",
      "Accuracy:  78.75  % Local:  75  epochs\n",
      "Accuracy:  65.625  % Global:  80  epochs\n",
      "Accuracy:  83.125  % Local:  80  epochs\n",
      "Accuracy:  73.75  % Global:  85  epochs\n",
      "Accuracy:  78.54166666666667  % Local:  85  epochs\n",
      "Accuracy:  67.08333333333334  % Global:  90  epochs\n",
      "Accuracy:  76.66666666666666  % Local:  90  epochs\n",
      "Accuracy:  68.54166666666666  % Global:  95  epochs\n",
      "Accuracy:  74.58333333333333  % Local:  95  epochs\n",
      "Accuracy:  71.25  % Global:  100  epochs\n",
      "Accuracy:  77.70833333333333  % Local:  100  epochs\n",
      "Accuracy:  65.20833333333333  % Global:  105  epochs\n",
      "Accuracy:  77.29166666666669  % Local:  105  epochs\n",
      "Accuracy:  73.33333333333334  % Global:  110  epochs\n",
      "Accuracy:  80.41666666666669  % Local:  110  epochs\n",
      "Accuracy:  66.25  % Global:  115  epochs\n",
      "Accuracy:  77.29166666666667  % Local:  115  epochs\n",
      "Accuracy:  72.29166666666667  % Global:  120  epochs\n",
      "Accuracy:  81.875  % Local:  120  epochs\n",
      "Accuracy:  74.16666666666667  % Global:  125  epochs\n",
      "Accuracy:  75.625  % Local:  125  epochs\n",
      "Accuracy:  72.70833333333333  % Global:  130  epochs\n",
      "Accuracy:  78.54166666666667  % Local:  130  epochs\n",
      "Accuracy:  77.91666666666667  % Global:  135  epochs\n",
      "Accuracy:  80.0  % Local:  135  epochs\n",
      "Accuracy:  78.95833333333333  % Global:  140  epochs\n",
      "Accuracy:  75.625  % Local:  140  epochs\n",
      "Accuracy:  67.91666666666667  % Global:  145  epochs\n",
      "Accuracy:  80.0  % Local:  145  epochs\n"
     ]
    }
   ],
   "source": [
    "accData=np.zeros((30,2))\n",
    "\n",
    "for i in range(0,150, 5):\n",
    "    e = i\n",
    "    r = 5\n",
    "    total = 0\n",
    "    for i in range(r):\n",
    "        one, two, three, four, five = modelRunTest(fd3, t3, e, 32, 0) \n",
    "        total+=one\n",
    "    print(\"Accuracy: \", total/r*100, \" % Global: \", e,\" epochs\")\n",
    "    accData[int(i/5),0]= total/r*100\n",
    "\n",
    "    total = 0 \n",
    "    for i in range(r):\n",
    "        one, two, three, four, five = modelRunTest(d3a, t3, e, 32, 0) \n",
    "        total+=one\n",
    "    print(\"Accuracy: \", total/r*100, \" % Local: \", e, \" epochs\")\n",
    "    accData[int(i/5),1]= total/r*100\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-9cecc8f07e1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelRun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpitches_List\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Fastball\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Breaking Ball\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Offspeed\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Other\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn_class\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# pitch by pitch testing, checking how the accuracy varies between different predicted and thrown pitches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msampleNum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "a, yhat, y_test, probs = modelRun(d4, 70, 32, 0)\n",
    "pitches_List = [\"Fastball\", \"Breaking Ball\", \"Offspeed\", \"Other\"]\n",
    "n_class =4\n",
    "# pitch by pitch testing, checking how the accuracy varies between different predicted and thrown pitches\n",
    "sampleNum, correct = 0,0\n",
    "print(pitches_List[0])\n",
    "for j in range(n_class):\n",
    "    print(pitches_List[j],\" Testing Thrown\")\n",
    "    for i in range(len(y_test)):\n",
    "        #print(y_test[i], yhat[i])\n",
    "        if(y_test[i]==j):\n",
    "            sampleNum+=1\n",
    "            if y_test[i]==yhat[i]:\n",
    "                correct+=1\n",
    "    if(sampleNum !=0 or correct !=0):\n",
    "        print(sampleNum, correct, correct/sampleNum*100, \"%\")\n",
    "    else:\n",
    "        print(\"Nan\")\n",
    "\n",
    "    sampleNum, correct = 0,0\n",
    "    \n",
    "\n",
    "\n",
    "for j in range(n_class):\n",
    "    print(pitches_List[j],\" Testing Predictions\")\n",
    "    for i in range(len(y_test)):\n",
    "        #print(y_test[i], yhat[i])\n",
    "        if(yhat[i]==j):\n",
    "            sampleNum+=1\n",
    "            if y_test[i]==yhat[i]:\n",
    "                correct+=1\n",
    "    if sampleNum !=0 and correct !=0:\n",
    "        print(sampleNum, correct, correct/sampleNum*100, \"%\")\n",
    "    else:\n",
    "        print(\"NaN\")\n",
    "\n",
    "    sampleNum, correct = 0,0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-229-f52c2a8979be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# split data into train and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.2, random_state=2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Game day testing, no retraining\n",
    "# split data into train and test sets\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.scale(X)\n",
    "X_train, X_test, y_train, y_test = X[100:], X[0:100], y[100:], y[0:100]\n",
    "#X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "n_class = len(unique(y))\n",
    "\n",
    "\n",
    "n_features=62\n",
    "model = Sequential()\n",
    "model.add(Dense(90, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(70, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"SGD\")\n",
    "# fit the keras model on the dataset\n",
    "\n",
    "\n",
    "history=model.fit(X_train, y_train, epochs=120, batch_size=16, verbose=0)\n",
    "\n",
    "\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.692\n",
      "Accuracy: 0.500\n",
      "Accuracy: 0.600\n",
      "Accuracy: 0.200\n",
      "Accuracy: 0.500\n",
      "Accuracy: 0.500\n",
      "Accuracy: 0.300\n",
      "Accuracy: 0.200\n",
      "Accuracy: 0.200\n",
      "Accuracy: 0.700\n",
      "Accuracy: 0.200\n"
     ]
    }
   ],
   "source": [
    "#Game day retraining, accuracy with retrains every ~ 10 pitches\n",
    "n_features =53\n",
    "model = Sequential()\n",
    "model.add(Dense(90, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "#model.add(Dense(40, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"SGD\")\n",
    "# fit the keras model on the dataset\n",
    "\n",
    "bs = 32\n",
    "e = 120\n",
    "v = 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[113:], X[100:113], y[113:], y[100:113]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[100:], X[90:100], y[100:], y[90:100]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[90:], X[80:90], y[90:], y[80:90]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[80:], X[70:80], y[80:], y[70:80]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[70:], X[60:70], y[70:], y[60:70]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[60:], X[50:60], y[60:], y[50:60]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[50:], X[40:50], y[50:], y[40:50]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[40:], X[30:40], y[40:], y[30:40]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[30:], X[20:30], y[30:], y[20:30]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[20:], X[10:20], y[20:], y[10:20]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[10:], X[0:10], y[10:], y[0:10]\n",
    "history=model.fit(X_train, y_train, epochs=e, batch_size=bs, verbose=v)\n",
    "# evaluate on test set\n",
    "yhat = model.predict(X_test)\n",
    "yhat = argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "innings = np.zeros((9,4))\n",
    "for j in range(len(y)):\n",
    "    innings[int(X[j,7])-1, int(y[j])]+=1\n",
    "print(innings)\n",
    "\n",
    "inningsPercent = innings\n",
    "for i in range(len(innings)):\n",
    "    inningsPercent[i]=innings[i]/np.sum(innings[i])\n",
    "print(inningsPercent)\n",
    "\n",
    "strikes = np.zeros((3,4))\n",
    "for j in range(len(y)):\n",
    "    strikes[int(X[j,2]), int(y[j])]+=1\n",
    "\n",
    "print(strikes)\n",
    "\n",
    "strikesPercent = strikes\n",
    "for i in range(len(strikes)):\n",
    "    strikesPercent[i]=strikes[i]/np.sum(strikes[i])\n",
    "print(strikesPercent)\n",
    "\n",
    "balls = np.zeros((4,4))\n",
    "for j in range(len(y)):\n",
    "    balls[int(X[j,1]), int(y[j])]+=1\n",
    "\n",
    "print(balls)\n",
    "\n",
    "ballsPercent = balls\n",
    "for i in range(len(balls)):\n",
    "    ballsPercent[i]=balls[i]/np.sum(balls[i])\n",
    "print(ballsPercent)\n",
    "\n",
    "pitchNum = np.zeros((12,4))\n",
    "for j in range(len(y)):\n",
    "    pitchNum[int(X[j,8]), int(y[j])]+=1\n",
    "\n",
    "print(pitchNum)\n",
    "\n",
    "pitchNumPercent = pitchNum\n",
    "for i in range(len(pitchNum)):\n",
    "    if( 0!=np.sum(pitchNum[i])):\n",
    "        pitchNumPercent[i]=pitchNum[i]/np.sum(pitchNum[i])\n",
    "print(pitchNumPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    #l = 32\n",
    "    data_X, data_y = b[:, 1:-1], b[:, -1]\n",
    "    data_X, data_y = data_X.astype('float'), data_y.astype('float')\n",
    "    n_features = data_X.shape[1]\n",
    "    data_X = preprocessing.scale(data_X)\n",
    "    #X_train, X_test, y_train, y_test = data_X[l:], data_X[0:l], data_y[l:], data_y[0:l]\n",
    "    X_train, X_test, y_train, y_test =train_test_split(data_X, data_y, test_size=0.2, random_state=2)\n",
    "\n",
    " d1a, d2a, d3a, d4a, d5a, d6a = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "da = [d1a, d2a, d3a, d4a, d5a, d6a]\n",
    "for i in range(6):\n",
    "    x,y = np.shape(d[i])\n",
    "    da[i] = np.zeros((x,y-2))\n",
    "    da[i][:,0:14] = d[i][:,0:14]\n",
    "    da[i][:,14:] = d[i][:,16:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
